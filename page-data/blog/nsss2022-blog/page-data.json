{
    "componentChunkName": "component---src-pages-blog-nsss-2022-blog-mdx",
    "path": "/blog/nsss2022-blog/",
    "result": {"pageContext":{"frontmatter":{"title":"Blog for IBM Neuro-Symbolic AI Summer School 2022 (8-9 August 2022)","description":"Page for IBM Neuro-Symbolic AI Summer School Blog","date":"15 August 2022"},"relativePagePath":"/blog/nsss2022-blog.mdx","titleType":"prepend","MdxNode":{"id":"d309aa41-ea58-53cb-8f16-8cb2b3fe5917","children":[],"parent":"1dd7dd64-ebf6-54b3-aaba-070529256789","internal":{"content":"---\ntitle: Blog for IBM Neuro-Symbolic AI Summer School 2022 (8-9 August 2022)\ndescription: Page for IBM Neuro-Symbolic AI Summer School Blog\ndate: 15 August 2022\n---\n\n## IBM Neuro-Symbolic AI Summer School 2022 - Playback\n<iframe src=\"https://video.ibm.com/combined-embed/playlist/655332\" style=\"border: 0;\" webkitallowfullscreen allowfullscreen frameborder=\"no\" width=\"800\" height=\"270\" referrerpolicy=\"no-referrer-when-downgrade\"></iframe>\n\n## Agenda & Registration\nhttps://ibm.biz/nsss2022\n\n\nIBM Neuro-Symbolic AI Summer School 2022 was held on 8 and 9 August 2022 (https://ibm.biz/nsss2022). It was held as a virtual event. The summer school had talks from over 25 IBMers in various areas of theory and the application of neuro-symbolic AI. **Artur d'Avila Garcez** - (City University of London) was the only external speaker and presented an overview of the field titled \"Neurosymbolic AI: The Third Wave\". The agenda was set to be a balance of educational content on neuro-symbolic AI and a discussion of recent results.\n\nWe are glad that the Neuro-symbolic AI community and in particular and broad AI community showed a great interest in the event. Over 6,000 people registered for the event and we had over 2,500 attendees in the live event. As of 24 Aug 2022, over 1,500 have accessed the recordings of the event. \n\nIn order not to miss any future events organized by us, please register at https://ibm.biz/nsss2022. You can also get a **free** digital badge for \"Neuro-Symbolic AI Essentials\" at https://www.credly.com/org/ibm/badge/neuro-symbolic-ai-essentials.\n\n---\n\n## Day 1 Session 1: Opening\n\n### Agenda\n- **Opening 20 minutes**\n   - Welcome (Alexander Gray - IBM)\n   - Motivation and Objective (Francesca Rossi - IBM)\n   - Summer School Overview (Jon Lenchner - IBM)\n   - Neuro-Symbolic AI Essentials Badge (Asim Munawar - IBM)\n- **Neurosymbolic AI: The Third Wave (Artur d'Avila Garcez - City University of London)** 1 hour\n- **Neuro-Symbolic AI Open Problems and IBM Progress (Alexander Gray - IBM)** 40 minutes\n\n### Summary\n????\n\n### Reference\n- Any blogs to read\n- Any papers\n- Any other material \n- Any Open source repo\n- ... ... ...\n\n---\n\n## Day 1 Session 2: Knowledge\n\n### Agenda\n- **Tutorial: Knowledge Foundations for AI Applications (Maria Chang - IBM)** 1 hour\n   - Knowledge Acquisition and Induction\n   - Semantic Web\n   - Logic for AI\n- **IBM Research Overview: Knowledge**\n   - Part 1: Universal Logic Knowledge Base (Rosario Uceda-Sosa - IBM) 25 minutes\n      - Interlinked KBs for broad encyclopedic, linguistic, and commonsense knowledge\n      - Supporting foundation for neuro-symbolic reasoning\n   - Part 2: ULKB Logic Language (Guilherme Lima - IBM) 25 minutes\n      - Higher order logic and simple type theory\n      - The ULKB Logic Language and its Python API\n   - Part 3: Deep linguistic processing (Alexandre Rademaker - IBM) 10 minutes\n      - Minimal recursive semantics and abstract meaning representation\n      - Open source tooling\n\n\n### Summary\n\nThe research overview is divided into three parts:\n- In part 2, we present the ULKB Logic Language, which is the language\n  interface to the ULKB knowledge graph.  We briefly discuss the foundations\n  and syntax of the ULKB Logic Language, and take a quick tour through its\n  Python API using code examples to illustrate its main features.\n\n### Reference\n\n---\n\n## Day 1 Session 3: Reasoning\n\n## Agenda\n\n- **Tutorial: A Very Brief Introduction to Logic and Reasoning (Achille Fokoue - IBM)** 1 hour\n   - First order logic (FOL) syntax and model theoretic semantics\n   - FOL reasoning and deductive systems\n   - FOL Extensions\n- **IBM Research Overview: Learnable Reasoning (Ndivhuwo Makondo - IBM, Hima Karanam - IBM)** 1 hour\n   - Overview of Learning to Reason (e.g., neural theorem provers, MLNs, LTNs, etc)\n   - Introduction to LNNs - our framework for Learnable Reasoning\n   - Applications of LNNS\n\n\n### Summary\nIn the tutorial, after introducing the syntax and model theoretic semantics of First Order Logic (FOL), we present the core reasoning tasks involved in FOL and the key desired properties (e.g., soundness, completeness) of deductive systems capable of performing those tasks. In particular, we describe a simple and concrete deductive system for FOL based on Resolution. Finally, we discuss important limitations of FOL and present standard extensions to address them.\n\n### Reference\n- Ronald Brachman and Hector Levesque. [Knowledge Representation and Reasoning](https://www.elsevier.com/books/knowledge-representation-and-reasoning/brachman/978-1-55860-932-7)\n---\n\n## Day 1 Session 4: Theory of Reasoning\n\n### Agenda\n\n- **Tutorial: Theory of Reasoning**\n   - Foundations of Reasoning with Classical Logic (Marco Carmosino - IBM) 30 minutes\n      - Desiderata: what is a logic, and what makes a logic \"good\"?\n      - Example: First-Order Logic on finite graphs.\n      - Game-based semantics for First-Order Logic\n   - Computational Complexity (Jon Lenchner - IBM) 30 minutes\n      - Time and Space Complexity: P vs. NP and Related Questions\n      - Descriptive Complexity\n      - Bridging from Descriptive Complexity to Time and Space Complexity via Games\n- **IBM Research Overview: Complexity**\n   - Part I: Theory of Real-Valued Logics (Ron Fagin - IBM) 30 minutes\n      - Allowing sentences to take values other than “true” or “false”\n      - A rich class of real-valued logic sentences\n      - A sound and complete axiomatization\n   - Part II: Games and Complexity Classes (Rik Sengupta - IBM) 30 minutes\n      - From Ehrenfecht-Fraisse Games to Multi-Structural Games\n      - From Multi-Structural Games to Syntactic Games\n      - Open Questions\n\n### Summary\n\n### Reference\n\n---\n\n## Day 2 Session 1: Machine Learning\n\n### Agenda\n\n- **Tutorial: What can Transformers do? (Mark Wegman - IBM, Hans Florian - IBM)** 1 hour 30 minutes\n   - Computational power of transformers\n   - What limits their power and what means there are around those limits?\n   - Theoretical power of a transformer and the relation to their behavior in practice\n- **IBM Research**\n   - Inductive Logic Programming with LNN (Prithviraj Sen - IBM, Sanjeeb Dash - IBM) 20 minutes\n      - Introduction to Inductive Logic Programming (ILP)\n      - Understanding LNN Output Semantics\n      - Generating LNNs for ILP\n      - Experimental Results: Knowledge Base Completion (KBC)\n   - NS architecture zoo (Tengfei Ma - IBM, Ronny Luss - IBM) 10 minutes\n      - LNN for Times Series\n      - LNN for Mixed Models\n\n### Summary\n\nThis session is divided into different parts:\n- In the second part, we take a closer look at how to learn rules in first-order logic from labeled data using logical neural networks. While inductive logic programming has for long pursued this goal, LNNs offer the advantage of being differentiable. Thus we can use gradient-based optimization to learn rules while ensuring that said rules adhere closely to first-order logic's semantics. We demonstrate LNN's effectiveness by reporting experimental results on real-world applications.  \n\n- In the third part, we consider different architectures that make use of LNNs. We first demonstrate an  architecture that uses LNN for time series, offering interpretability beyond other time series models, and apply this to biological data regarding wound healing. Next we demonstrate how different modalities can be incorporated into LNN models using additional neural network layers. An example using MNIST images is shown.\n\n### Reference\n- Prithviraj Sen, Breno W. S. R. de Carvalho, Ryan Riegel, and Alexander Gray. [Neuro-Symbolic Inductive Logic Programming with Logical Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/20795) in Proceedings of the AAAI Conference on Artificial Intelligence, 36(8), 8212-8219, Vancouver, Canada, 2022.\n- Ruixuan Yan; Agung Julius; Maria Chang; Achille Fokoue; Tengfei Ma; Rosario Uceda-Sosa. [STONE: Signal Temporal Logic Neural Network for Time Series Classification](https://ieeexplore.ieee.org/abstract/document/9679844?casa_token=vSlG0yvY87EAAAAA:jyVtQ5Hdm-kDzIv1wvFjn5W9HND5N0XadfYjwtEa7M0UfkptvCo6FGvmf0jG6mxNyO7cyGRd). In 2021 International Conference on Data Mining Workshops (ICDMW)\n\n---\n\n## Day 2 Session 2: NLP via Logic\n\n### Agenda\n\n- Tutorial: NLP via Logic\n   - Deep Semantic Parsing with Abstract Meaning Representation (Ramon Astudillo - IBM) 30 minutes\n      - AMR as Deep Semantic Representation\n      - AMR parsing and AMR-to-text machine learning approaches\n      - Incorporating structure to Large Language Models for AMR parsing\n   - Entity Linking (Dinesh Garg - IBM) 30 minutes\n      - Setup: What do we mean by entity, mention, and linking\n      - Linking over knowledge graphs\n      - Linking over relational databases\n- **Challenges and Approaches for Reliable Reasoning with Foundation Models: An Abstractive Summarization Use-case (Pavan Kapanipathi - IBM, Hans Florian - IBM)** 1 hour\n   - Reliable Reasoning with Foundation Models\n   - Abstractive Summarization: Reasoning and Factuality in Summarization\n   - Challenges and Neuro-Symbolic Approaches\n\n### Summary\n\n### Reference\n\n- https://github.com/IBM/transition-amr-parser\n- Zhou J, Naseem T, Astudillo RF, Lee YS, Florian R, Roukos S. Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing. InProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing 2021 Nov (pp. 6279-6290).\n\n---\n\n## Day 2 Session 3: Sequential Decision Making\n\n### Agenda\n\n- **Tutorial: SDM**\n   - Theory and practice of RL (Miao Liu - IBM) 30 minutes\n      - Core elements in RL\n      - Computational approaches and RL tools\n      - Important mechanisms - Hierarchical RL and Multiagent RL\n   - Theory and practice of AI Planning (Michael Katz - IBM) 30 minutes\n      - What is planning and why is it hard\n      - Computational approaches to classical planning\n      - Planners and planning tools\n- **IBM Research Overview: SDM**\n   - Integrating Planning and RL (Junkyu Lee - IBM) 30 minutes\n      - Introduction to integrating planning and RL\n      - AI Planning as annotation of RL\n      - Demonstration of libraries for planning annotated RL tasks\n   - Logical optimal actions (Don Joven - IBM, Maxwell Crouse - IBM) 30 minutes\n      - Text-based games as an application environment\n      - LNN rule induction for learning world models\n      - The situation calculus and theorem proving applied to planning\n\n### Summary\n\nThe section on sequential decision making consisted of two parts. \nFirst, turorials on the two major approaches to sequential decision making were given, introducing the audience to reinforcement learning (RL) and AI planning.\nThen, two presentations on IBM research contributions that integrate the two approaches to sequential decision making were presented.\nPlanning annotated RL (PaRL) allows for efficient sequential decision making in complex combinatorial domains, by performing high-level decision making with planning and low-level with RL.\nLogical optimal action (LOA) focuses on learning action models for planning and ways to exploit the knowledge.\n\n### Reference\n- [AI Planning tutorial at AAAI 2022](https://aiplanning-tutorial.github.io/)\n- [AI Planning Service](https://github.com/ibm/aiplanningservice)\n- [ForbidIterative planners suite](https://github.com/ibm/forbiditerative)\n- [PRL Workshop – Bridging the Gap Between AI Planning and Reinforcement Learning](https://prl-theworkshop.github.io/)\n- M. Katz, S. Sohrabi, O. Udrea, D. Winterer, [A Novel Iterative Approach to Top-k Planning](https://ctpelok77.github.io/papers/icaps2018a.pdf), in Proceedings of The 28th International Conference on Automated Planning and Scheduling (ICAPS), Delft, Netherlands, 2018.\n- M. Katz, S. Sohrabi, [Reshaping Diverse Planning](https://ctpelok77.github.io/papers/aaai2020b.pdf), in Proceedings of The 34th AAAI Conference on Artificial Intelligence (AAAI), New York, NY, USA, 2020. \n- M. Katz, S. Sohrabi, O. Udrea, [Top-Quality Planning: Finding Practically Useful Sets of Best Plans](https://ctpelok77.github.io/papers/aaai2020c.pdf), in Proceedings of The 34th AAAI Conference on Artificial Intelligence (AAAI), New York, NY, USA, 2020. \n- M. Katz, S. Sohrabi, O. Udrea, [Bounding Quality in Diverse Planning](https://ctpelok77.github.io/papers/aaai2022a.pdf), in Proceedings of The 36th AAAI Conference on Artificial Intelligence (AAAI), Virtual, 2022.\n- M. Katz and S. Sohrabi, [Who Needs These Operators Anyway: Top Quality Planning with Operator Subset Criteria](https://ctpelok77.github.io/papers/icaps2022a.pdf), in Proceedings of The 32nd International Conference on Automated Planning and Scheduling (ICAPS), Virtual, 2022. \n- S. Sievers, M. Katz, S. Sohrabi, H. Samulowitz, P. Ferber, [Deep learning for cost-optimal planning: Task-dependent planner selection](https://ctpelok77.github.io/papers/aaai2019a.pdf), in Proceedings of The 33rd AAAI Conference on Artificial Intelligence (AAAI), Honolulu, HI, USA, 2019. \n- D. Speck, M. Katz, [Symbolic Search for Oversubscription Planning](https://ctpelok77.github.io/papers/aaai2021.pdf), in Proceedings of The 35th AAAI Conference on Artificial Intelligence (AAAI), Virtual, 2021.\n- M. Katz, E. Keyder, [A* Search and Bound-Sensitive Heuristics for Oversubscription Planning](https://ctpelok77.github.io/papers/aaai2022b.pdf), in Proceedings of The 36th AAAI Conference on Artificial Intelligence (AAAI), Virtual, 2022.\n- M. Katz, P. Ram, S. Sohrabi, O. Udrea, [Exploring Context-Free Languages via Planning: The Case for Automating Machine Learning](https://ctpelok77.github.io/papers/icaps2020.pdf), in Proceedings of The 30th International Conference on Automated Planning and Scheduling (ICAPS), Nancy, France, 2020.\n- J. Lee, M. Katz, D. J. Agravante, M. Liu, T. Klinger, M. Campbell, S. Sohrabi and G. Tesauro, [AI Planning Annotation in Reinforcement Learning: Options and Beyond](https://prl-theworkshop.github.io/prl2021/papers/PRL2021_paper_36.pdf), in ICAPS 2021 Workshop on Bridging the Gap Between AI Planning and Reinforcement Learning (PRL), Virtual, 2021.\n- M. Abdulhai, D. K. Kim, M. Riemer, M. Liu, G. Tesauro, J. P. How. Context-specific representation abstraction for deep option learning, In Proc. of The 36th AAAI Conference on Artificial Intelligence (AAAI), 2022\n- D.K. Kim, M. Liu, M. Riemer, C.C. Sun, M. Abdulhai, G. Habibi, S. Lopez-Cot, G. Tesauro, and J.P. How. Learning Hierarchical Teaching Policies for Cooperative Agents., in Proc. of the 38th International Conference on Machine Learning (ICML), 2021\n- D.K. Kim, M. Liu, S. Omidshafiei, S. Lopez-Cot, M. Riemer, G. Habibi, G. Tesauro, S. Mourad, M. Campbell and J.P. How. Learning Hierarchical Teaching Policies for Cooperative Agents. in Proc. of The 19th International Conference on Autonomous Agents and Multiagent Systems:(AAMAS), 2020\n- M. Riemer, I. Cases, C. Rosenbaum, M. Liu, and G. Tesauro. On the Role of Weights Sharing. In Proc. of The 34th AAAI Conference on Artificial Intelligence (AAAI), 2020\n- S. Omidshafiei, D.K. Kim, M. Liu, G. Tesauro, M. Riemer, C. Amato, M. Campbell, and J.P.How. Learning to Teach in Cooperative Multiagent Reinforcement Learning. in the Proc. of The 33th AAAI Conference on Artificial Intelligence (AAAI), 2021\n- M. Riemer,M. Liu and G. Tesauro. Learning Abstract Options. In The Proc. of Neural Information Processing Systems (Neurips), 2018\n- M. Liu, K. Sivakumar, S. Omidshafiei, C. Amato and J. P. How. Learning for Multi-robot Cooperation in Partially Observable Stochastic Environments with Macro-actions, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2017\n\n---\n\n## Day 2 Session 4: Neuro-Symbolic AI toolkit/Closing\n\n### Agenda\n\n- **Neuro-symbolic AI Toolkit (Naweed Khan - IBM)** 45 minutes\n   - Logical Neural Networks (LNN)\n   - Universal Logic Knowledge Base (ULKB)\n   - Additional NSTK Components\n- **Closing** 15 minutes\n   - Badge, Feedback (Asim Munawar - IBM)\n   - Closing Remarks (Alexander Gray - IBM)\n\n### Summary\n\nIn this final session, we conclude the summer school by reiterating the goals of Neuro-Symbolic AI and reflecting on how complex problems can be approached using \nvarious components that have been presented on throughout the event. IBM Research remains commited to open-source collaboration and upskilling, which is reflected in the release of the [Neuro-Symbolic AI Toolkit](https://ibm.github.io/neuro-symbolic-ai/toolkit/) and the [Neuro-Symbolic AI Essentials Badge](https://www.credly.com/org/ibm/badge/neuro-symbolic-ai-essentials). We hope that you enjoyed the summer school and that you will reach out to the various authors on their respective work. Let's create.\n","type":"Mdx","contentDigest":"1d15708eb2b8f01c26cedea07ddad571","owner":"gatsby-plugin-mdx","counter":152,"fieldOwners":{"source":"gatsby-plugin-mdx-source-name"}},"frontmatter":{"title":"Blog for IBM Neuro-Symbolic AI Summer School 2022 (8-9 August 2022)","description":"Page for IBM Neuro-Symbolic AI Summer School Blog","date":"15 August 2022"},"exports":{},"rawBody":"---\ntitle: Blog for IBM Neuro-Symbolic AI Summer School 2022 (8-9 August 2022)\ndescription: Page for IBM Neuro-Symbolic AI Summer School Blog\ndate: 15 August 2022\n---\n\n## IBM Neuro-Symbolic AI Summer School 2022 - Playback\n<iframe src=\"https://video.ibm.com/combined-embed/playlist/655332\" style=\"border: 0;\" webkitallowfullscreen allowfullscreen frameborder=\"no\" width=\"800\" height=\"270\" referrerpolicy=\"no-referrer-when-downgrade\"></iframe>\n\n## Agenda & Registration\nhttps://ibm.biz/nsss2022\n\n\nIBM Neuro-Symbolic AI Summer School 2022 was held on 8 and 9 August 2022 (https://ibm.biz/nsss2022). It was held as a virtual event. The summer school had talks from over 25 IBMers in various areas of theory and the application of neuro-symbolic AI. **Artur d'Avila Garcez** - (City University of London) was the only external speaker and presented an overview of the field titled \"Neurosymbolic AI: The Third Wave\". The agenda was set to be a balance of educational content on neuro-symbolic AI and a discussion of recent results.\n\nWe are glad that the Neuro-symbolic AI community and in particular and broad AI community showed a great interest in the event. Over 6,000 people registered for the event and we had over 2,500 attendees in the live event. As of 24 Aug 2022, over 1,500 have accessed the recordings of the event. \n\nIn order not to miss any future events organized by us, please register at https://ibm.biz/nsss2022. You can also get a **free** digital badge for \"Neuro-Symbolic AI Essentials\" at https://www.credly.com/org/ibm/badge/neuro-symbolic-ai-essentials.\n\n---\n\n## Day 1 Session 1: Opening\n\n### Agenda\n- **Opening 20 minutes**\n   - Welcome (Alexander Gray - IBM)\n   - Motivation and Objective (Francesca Rossi - IBM)\n   - Summer School Overview (Jon Lenchner - IBM)\n   - Neuro-Symbolic AI Essentials Badge (Asim Munawar - IBM)\n- **Neurosymbolic AI: The Third Wave (Artur d'Avila Garcez - City University of London)** 1 hour\n- **Neuro-Symbolic AI Open Problems and IBM Progress (Alexander Gray - IBM)** 40 minutes\n\n### Summary\n????\n\n### Reference\n- Any blogs to read\n- Any papers\n- Any other material \n- Any Open source repo\n- ... ... ...\n\n---\n\n## Day 1 Session 2: Knowledge\n\n### Agenda\n- **Tutorial: Knowledge Foundations for AI Applications (Maria Chang - IBM)** 1 hour\n   - Knowledge Acquisition and Induction\n   - Semantic Web\n   - Logic for AI\n- **IBM Research Overview: Knowledge**\n   - Part 1: Universal Logic Knowledge Base (Rosario Uceda-Sosa - IBM) 25 minutes\n      - Interlinked KBs for broad encyclopedic, linguistic, and commonsense knowledge\n      - Supporting foundation for neuro-symbolic reasoning\n   - Part 2: ULKB Logic Language (Guilherme Lima - IBM) 25 minutes\n      - Higher order logic and simple type theory\n      - The ULKB Logic Language and its Python API\n   - Part 3: Deep linguistic processing (Alexandre Rademaker - IBM) 10 minutes\n      - Minimal recursive semantics and abstract meaning representation\n      - Open source tooling\n\n\n### Summary\n\nThe research overview is divided into three parts:\n- In part 2, we present the ULKB Logic Language, which is the language\n  interface to the ULKB knowledge graph.  We briefly discuss the foundations\n  and syntax of the ULKB Logic Language, and take a quick tour through its\n  Python API using code examples to illustrate its main features.\n\n### Reference\n\n---\n\n## Day 1 Session 3: Reasoning\n\n## Agenda\n\n- **Tutorial: A Very Brief Introduction to Logic and Reasoning (Achille Fokoue - IBM)** 1 hour\n   - First order logic (FOL) syntax and model theoretic semantics\n   - FOL reasoning and deductive systems\n   - FOL Extensions\n- **IBM Research Overview: Learnable Reasoning (Ndivhuwo Makondo - IBM, Hima Karanam - IBM)** 1 hour\n   - Overview of Learning to Reason (e.g., neural theorem provers, MLNs, LTNs, etc)\n   - Introduction to LNNs - our framework for Learnable Reasoning\n   - Applications of LNNS\n\n\n### Summary\nIn the tutorial, after introducing the syntax and model theoretic semantics of First Order Logic (FOL), we present the core reasoning tasks involved in FOL and the key desired properties (e.g., soundness, completeness) of deductive systems capable of performing those tasks. In particular, we describe a simple and concrete deductive system for FOL based on Resolution. Finally, we discuss important limitations of FOL and present standard extensions to address them.\n\n### Reference\n- Ronald Brachman and Hector Levesque. [Knowledge Representation and Reasoning](https://www.elsevier.com/books/knowledge-representation-and-reasoning/brachman/978-1-55860-932-7)\n---\n\n## Day 1 Session 4: Theory of Reasoning\n\n### Agenda\n\n- **Tutorial: Theory of Reasoning**\n   - Foundations of Reasoning with Classical Logic (Marco Carmosino - IBM) 30 minutes\n      - Desiderata: what is a logic, and what makes a logic \"good\"?\n      - Example: First-Order Logic on finite graphs.\n      - Game-based semantics for First-Order Logic\n   - Computational Complexity (Jon Lenchner - IBM) 30 minutes\n      - Time and Space Complexity: P vs. NP and Related Questions\n      - Descriptive Complexity\n      - Bridging from Descriptive Complexity to Time and Space Complexity via Games\n- **IBM Research Overview: Complexity**\n   - Part I: Theory of Real-Valued Logics (Ron Fagin - IBM) 30 minutes\n      - Allowing sentences to take values other than “true” or “false”\n      - A rich class of real-valued logic sentences\n      - A sound and complete axiomatization\n   - Part II: Games and Complexity Classes (Rik Sengupta - IBM) 30 minutes\n      - From Ehrenfecht-Fraisse Games to Multi-Structural Games\n      - From Multi-Structural Games to Syntactic Games\n      - Open Questions\n\n### Summary\n\n### Reference\n\n---\n\n## Day 2 Session 1: Machine Learning\n\n### Agenda\n\n- **Tutorial: What can Transformers do? (Mark Wegman - IBM, Hans Florian - IBM)** 1 hour 30 minutes\n   - Computational power of transformers\n   - What limits their power and what means there are around those limits?\n   - Theoretical power of a transformer and the relation to their behavior in practice\n- **IBM Research**\n   - Inductive Logic Programming with LNN (Prithviraj Sen - IBM, Sanjeeb Dash - IBM) 20 minutes\n      - Introduction to Inductive Logic Programming (ILP)\n      - Understanding LNN Output Semantics\n      - Generating LNNs for ILP\n      - Experimental Results: Knowledge Base Completion (KBC)\n   - NS architecture zoo (Tengfei Ma - IBM, Ronny Luss - IBM) 10 minutes\n      - LNN for Times Series\n      - LNN for Mixed Models\n\n### Summary\n\nThis session is divided into different parts:\n- In the second part, we take a closer look at how to learn rules in first-order logic from labeled data using logical neural networks. While inductive logic programming has for long pursued this goal, LNNs offer the advantage of being differentiable. Thus we can use gradient-based optimization to learn rules while ensuring that said rules adhere closely to first-order logic's semantics. We demonstrate LNN's effectiveness by reporting experimental results on real-world applications.  \n\n- In the third part, we consider different architectures that make use of LNNs. We first demonstrate an  architecture that uses LNN for time series, offering interpretability beyond other time series models, and apply this to biological data regarding wound healing. Next we demonstrate how different modalities can be incorporated into LNN models using additional neural network layers. An example using MNIST images is shown.\n\n### Reference\n- Prithviraj Sen, Breno W. S. R. de Carvalho, Ryan Riegel, and Alexander Gray. [Neuro-Symbolic Inductive Logic Programming with Logical Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/20795) in Proceedings of the AAAI Conference on Artificial Intelligence, 36(8), 8212-8219, Vancouver, Canada, 2022.\n- Ruixuan Yan; Agung Julius; Maria Chang; Achille Fokoue; Tengfei Ma; Rosario Uceda-Sosa. [STONE: Signal Temporal Logic Neural Network for Time Series Classification](https://ieeexplore.ieee.org/abstract/document/9679844?casa_token=vSlG0yvY87EAAAAA:jyVtQ5Hdm-kDzIv1wvFjn5W9HND5N0XadfYjwtEa7M0UfkptvCo6FGvmf0jG6mxNyO7cyGRd). In 2021 International Conference on Data Mining Workshops (ICDMW)\n\n---\n\n## Day 2 Session 2: NLP via Logic\n\n### Agenda\n\n- Tutorial: NLP via Logic\n   - Deep Semantic Parsing with Abstract Meaning Representation (Ramon Astudillo - IBM) 30 minutes\n      - AMR as Deep Semantic Representation\n      - AMR parsing and AMR-to-text machine learning approaches\n      - Incorporating structure to Large Language Models for AMR parsing\n   - Entity Linking (Dinesh Garg - IBM) 30 minutes\n      - Setup: What do we mean by entity, mention, and linking\n      - Linking over knowledge graphs\n      - Linking over relational databases\n- **Challenges and Approaches for Reliable Reasoning with Foundation Models: An Abstractive Summarization Use-case (Pavan Kapanipathi - IBM, Hans Florian - IBM)** 1 hour\n   - Reliable Reasoning with Foundation Models\n   - Abstractive Summarization: Reasoning and Factuality in Summarization\n   - Challenges and Neuro-Symbolic Approaches\n\n### Summary\n\n### Reference\n\n- https://github.com/IBM/transition-amr-parser\n- Zhou J, Naseem T, Astudillo RF, Lee YS, Florian R, Roukos S. Structure-aware Fine-tuning of Sequence-to-sequence Transformers for Transition-based AMR Parsing. InProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing 2021 Nov (pp. 6279-6290).\n\n---\n\n## Day 2 Session 3: Sequential Decision Making\n\n### Agenda\n\n- **Tutorial: SDM**\n   - Theory and practice of RL (Miao Liu - IBM) 30 minutes\n      - Core elements in RL\n      - Computational approaches and RL tools\n      - Important mechanisms - Hierarchical RL and Multiagent RL\n   - Theory and practice of AI Planning (Michael Katz - IBM) 30 minutes\n      - What is planning and why is it hard\n      - Computational approaches to classical planning\n      - Planners and planning tools\n- **IBM Research Overview: SDM**\n   - Integrating Planning and RL (Junkyu Lee - IBM) 30 minutes\n      - Introduction to integrating planning and RL\n      - AI Planning as annotation of RL\n      - Demonstration of libraries for planning annotated RL tasks\n   - Logical optimal actions (Don Joven - IBM, Maxwell Crouse - IBM) 30 minutes\n      - Text-based games as an application environment\n      - LNN rule induction for learning world models\n      - The situation calculus and theorem proving applied to planning\n\n### Summary\n\nThe section on sequential decision making consisted of two parts. \nFirst, turorials on the two major approaches to sequential decision making were given, introducing the audience to reinforcement learning (RL) and AI planning.\nThen, two presentations on IBM research contributions that integrate the two approaches to sequential decision making were presented.\nPlanning annotated RL (PaRL) allows for efficient sequential decision making in complex combinatorial domains, by performing high-level decision making with planning and low-level with RL.\nLogical optimal action (LOA) focuses on learning action models for planning and ways to exploit the knowledge.\n\n### Reference\n- [AI Planning tutorial at AAAI 2022](https://aiplanning-tutorial.github.io/)\n- [AI Planning Service](https://github.com/ibm/aiplanningservice)\n- [ForbidIterative planners suite](https://github.com/ibm/forbiditerative)\n- [PRL Workshop – Bridging the Gap Between AI Planning and Reinforcement Learning](https://prl-theworkshop.github.io/)\n- M. Katz, S. Sohrabi, O. Udrea, D. Winterer, [A Novel Iterative Approach to Top-k Planning](https://ctpelok77.github.io/papers/icaps2018a.pdf), in Proceedings of The 28th International Conference on Automated Planning and Scheduling (ICAPS), Delft, Netherlands, 2018.\n- M. Katz, S. Sohrabi, [Reshaping Diverse Planning](https://ctpelok77.github.io/papers/aaai2020b.pdf), in Proceedings of The 34th AAAI Conference on Artificial Intelligence (AAAI), New York, NY, USA, 2020. \n- M. Katz, S. Sohrabi, O. Udrea, [Top-Quality Planning: Finding Practically Useful Sets of Best Plans](https://ctpelok77.github.io/papers/aaai2020c.pdf), in Proceedings of The 34th AAAI Conference on Artificial Intelligence (AAAI), New York, NY, USA, 2020. \n- M. Katz, S. Sohrabi, O. Udrea, [Bounding Quality in Diverse Planning](https://ctpelok77.github.io/papers/aaai2022a.pdf), in Proceedings of The 36th AAAI Conference on Artificial Intelligence (AAAI), Virtual, 2022.\n- M. Katz and S. Sohrabi, [Who Needs These Operators Anyway: Top Quality Planning with Operator Subset Criteria](https://ctpelok77.github.io/papers/icaps2022a.pdf), in Proceedings of The 32nd International Conference on Automated Planning and Scheduling (ICAPS), Virtual, 2022. \n- S. Sievers, M. Katz, S. Sohrabi, H. Samulowitz, P. Ferber, [Deep learning for cost-optimal planning: Task-dependent planner selection](https://ctpelok77.github.io/papers/aaai2019a.pdf), in Proceedings of The 33rd AAAI Conference on Artificial Intelligence (AAAI), Honolulu, HI, USA, 2019. \n- D. Speck, M. Katz, [Symbolic Search for Oversubscription Planning](https://ctpelok77.github.io/papers/aaai2021.pdf), in Proceedings of The 35th AAAI Conference on Artificial Intelligence (AAAI), Virtual, 2021.\n- M. Katz, E. Keyder, [A* Search and Bound-Sensitive Heuristics for Oversubscription Planning](https://ctpelok77.github.io/papers/aaai2022b.pdf), in Proceedings of The 36th AAAI Conference on Artificial Intelligence (AAAI), Virtual, 2022.\n- M. Katz, P. Ram, S. Sohrabi, O. Udrea, [Exploring Context-Free Languages via Planning: The Case for Automating Machine Learning](https://ctpelok77.github.io/papers/icaps2020.pdf), in Proceedings of The 30th International Conference on Automated Planning and Scheduling (ICAPS), Nancy, France, 2020.\n- J. Lee, M. Katz, D. J. Agravante, M. Liu, T. Klinger, M. Campbell, S. Sohrabi and G. Tesauro, [AI Planning Annotation in Reinforcement Learning: Options and Beyond](https://prl-theworkshop.github.io/prl2021/papers/PRL2021_paper_36.pdf), in ICAPS 2021 Workshop on Bridging the Gap Between AI Planning and Reinforcement Learning (PRL), Virtual, 2021.\n- M. Abdulhai, D. K. Kim, M. Riemer, M. Liu, G. Tesauro, J. P. How. Context-specific representation abstraction for deep option learning, In Proc. of The 36th AAAI Conference on Artificial Intelligence (AAAI), 2022\n- D.K. Kim, M. Liu, M. Riemer, C.C. Sun, M. Abdulhai, G. Habibi, S. Lopez-Cot, G. Tesauro, and J.P. How. Learning Hierarchical Teaching Policies for Cooperative Agents., in Proc. of the 38th International Conference on Machine Learning (ICML), 2021\n- D.K. Kim, M. Liu, S. Omidshafiei, S. Lopez-Cot, M. Riemer, G. Habibi, G. Tesauro, S. Mourad, M. Campbell and J.P. How. Learning Hierarchical Teaching Policies for Cooperative Agents. in Proc. of The 19th International Conference on Autonomous Agents and Multiagent Systems:(AAMAS), 2020\n- M. Riemer, I. Cases, C. Rosenbaum, M. Liu, and G. Tesauro. On the Role of Weights Sharing. In Proc. of The 34th AAAI Conference on Artificial Intelligence (AAAI), 2020\n- S. Omidshafiei, D.K. Kim, M. Liu, G. Tesauro, M. Riemer, C. Amato, M. Campbell, and J.P.How. Learning to Teach in Cooperative Multiagent Reinforcement Learning. in the Proc. of The 33th AAAI Conference on Artificial Intelligence (AAAI), 2021\n- M. Riemer,M. Liu and G. Tesauro. Learning Abstract Options. In The Proc. of Neural Information Processing Systems (Neurips), 2018\n- M. Liu, K. Sivakumar, S. Omidshafiei, C. Amato and J. P. How. Learning for Multi-robot Cooperation in Partially Observable Stochastic Environments with Macro-actions, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2017\n\n---\n\n## Day 2 Session 4: Neuro-Symbolic AI toolkit/Closing\n\n### Agenda\n\n- **Neuro-symbolic AI Toolkit (Naweed Khan - IBM)** 45 minutes\n   - Logical Neural Networks (LNN)\n   - Universal Logic Knowledge Base (ULKB)\n   - Additional NSTK Components\n- **Closing** 15 minutes\n   - Badge, Feedback (Asim Munawar - IBM)\n   - Closing Remarks (Alexander Gray - IBM)\n\n### Summary\n\nIn this final session, we conclude the summer school by reiterating the goals of Neuro-Symbolic AI and reflecting on how complex problems can be approached using \nvarious components that have been presented on throughout the event. IBM Research remains commited to open-source collaboration and upskilling, which is reflected in the release of the [Neuro-Symbolic AI Toolkit](https://ibm.github.io/neuro-symbolic-ai/toolkit/) and the [Neuro-Symbolic AI Essentials Badge](https://www.credly.com/org/ibm/badge/neuro-symbolic-ai-essentials). We hope that you enjoyed the summer school and that you will reach out to the various authors on their respective work. Let's create.\n","fileAbsolutePath":"/home/travis/build/IBM-Research-AI/neuro-symbolic-ai-toolkit-site/src/pages/blog/nsss2022-blog.mdx","fields":{"source":"blog"},"__gatsby_resolved":{"fields":{"source":"blog"},"frontmatter":{"date":"15 August 2022"},"slug":"nsss2022-blog"}}}},
    "staticQueryHashes": ["1008643715","1364590287","137577622","1404468418","151170173","1794870524","2102389209","2746626797","3018647132","3037994772","3151510810","768070550"]}